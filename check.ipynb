{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Making Large Language Models produce structured output using Guardrails AI\"\n",
    "jupyter: python3\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "filters:\n",
    "  - line-highlight\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import TextWrapper\n",
    "\n",
    "wrapper = TextWrapper(width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models are hard to configure and control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a very simple prompt asking OpenAI's GPT-3 to add 1+1 and return the answer. In this case, we would like GPT-3 to act as a calculator and return back to us the result. \n",
    "\n",
    "Note for those not familiar with how to call OpenAI chat completion: the TLDR is you choose a model, send the prompts as history of system, user and assistant messages, along with somple sampling parameters like (`top_p` or `temperature`) and then you get back a response which you can parse the assistant message from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The integer answer to 1+1 is 2.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: TRUE\n",
    "#| eval: FALSE\n",
    "#| source-line-numbers: 6\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"user\", \"content\": \"Return only the integer answer, 1+1=\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response['choices'][0][\"message\"]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even though we explicitly stated \"Return only the integer answer\" expecting only `2` to be returned, the model chose to return a full sentence string instead, causing all sorts of frustration. \n",
    "\n",
    "To resolve this naively, we would then go about a \"prompt engineering\" journey where we try to find the \"right prompt template\" to get the model to do what we want. This is a very time consuming process and is not scalable.\n",
    "\n",
    "So the question becomes, when engineering a system that makes use of a language model as one component, how can we enforce control over the output of the model without spending endless cycles tuning prompts ?\n",
    "\n",
    "There are several approaches to this probelm. \n",
    "\n",
    "In this article, we will do a deep dive into the [guardrails AI](https://guardrailsai.com) approach. The TLDR is the following: guardrails AI\n",
    "- allows us to declare our output schema using familiar tooling like [pydantic](https://docs.pydantic.dev/latest/)\n",
    "- provides us with out of the box prompt templates that we can use to get the model to produce the output we want\n",
    "- if the language model fails to produce the output we want, guardrails has out of the box prompt templates to \"re-ask the model\" to correct its output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails AI deep dive\n",
    "\n",
    "### De-mystifying the guardrails AI approach with a simple example\n",
    "We start with the same simple query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=?\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"1+1=?\"\"\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the desired answer schema using [pydantic]((https://docs.pydantic.dev/latest/). Pydantic relies on python type annotations to define the attributes of a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class IntegerAnswer(BaseModel):\n",
    "    \"\"\"The answer to a question.\"\"\"\n",
    "\n",
    "    value: int = Field(description=\"The answer to the question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have to define quite a few things to get guardrails AI to work out of the box:\n",
    "- the system (instruction) prompt template\n",
    "- the user prompt template\n",
    "\n",
    "Call guardrails `Guard.from_pydantic` to \n",
    "-  produce a spec of our pydantic model that can be inserted into the prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "\n",
    "system_instructions = \"\"\"\n",
    "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "${query}\n",
    "\n",
    "${gr.complete_json_suffix_v2}\n",
    "\"\"\"\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    instructions=system_instructions,\n",
    "    prompt=user_prompt,\n",
    "    output_class=IntegerAnswer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the produced system prompt and user prompt to see how the spec is inserted into the templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system prompt \n",
      "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
      "\n",
      "user prompt \n",
      "${query}\n",
      "\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "<output>\n",
      "    <integer name=\"value\" description=\"The answer to the question.\"/>\n",
      "</output>\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"system prompt\", guard.instructions)\n",
    "print(f\"user prompt\", guard.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To craft our prompt-templates, we need to get familiarized with Guardrails AI's prompt templating language.\n",
    "\n",
    "- guardrails relies on a `${var}` syntax\n",
    "- variables like our `query` can be passed in like so `${query}` \n",
    "- constants like `complete_json_suffix_v2`  reference pre-defined prompt templates which we can find in the Guardrails AI [constants.xml](https://github.com/guardrails-ai/guardrails/blob/main/guardrails/constants.xml) file\n",
    "\n",
    "\n",
    "We can see that Guardrails AI has produced the following xml spec from our pydantic model\n",
    "```xml\n",
    "<output>\n",
    "    <integer name=\"value\" description=\"The answer to the question.\"/>\n",
    "</output>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting how Guardrail builds the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the `constants.xml` file and see what the `complete_json_suffix_v2` template looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "${output_schema}\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse((Path(gd.__file__).parent / \"constants.xml\"))\n",
    "root = tree.getroot()\n",
    "\n",
    "# Now you can access the elements in the XML file\n",
    "for child in root:\n",
    "    if child.tag == \"complete_json_suffix_v2\":\n",
    "        print(child.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the same prompt as before but we can see `${output_schema}` is not populated with a schema yet.\n",
    "\n",
    "so we can deduce that the Guardrails's `Guard` is responsible for:\n",
    "- producing the `output_schema` from the pydantic model\n",
    "- replacing `${output_schema}` with the produced schema\n",
    "\n",
    "Let's inspect the `Guard` class to see what it is composed from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrail\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mguardrails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRail\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_reasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbase_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "The Guard class.\n",
      "\n",
      "This class is the main entry point for using Guardrails. It is\n",
      "initialized from one of the following class methods:\n",
      "\n",
      "- `from_rail`\n",
      "- `from_rail_string`\n",
      "- `from_pydantic`\n",
      "- `from_string`\n",
      "\n",
      "The `__call__`\n",
      "method functions as a wrapper around LLM APIs. It takes in an LLM\n",
      "API, and optional prompt parameters, and returns the raw output from\n",
      "the LLM and the validated output.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize the Guard.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/3.10.8/envs/structured-output-llm-py310/lib/python3.10/site-packages/guardrails/guard.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "gd.Guard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `Guard` object is composed of:\n",
    "\n",
    "- a `Rail` object\n",
    "- a setting `num_reasks` for how many attempts to re-ask the model in case of a failure\n",
    "- the pydantic `base_model`\n",
    "    \n",
    "\n",
    "Looking at `Guard.from_pydantic` we can see that it is constructing the `Rail` object from the base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pydantic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minstructions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_reasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Guard\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Create a Guard instance from a Pydantic model and prompt.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydantic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutput_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_reasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_reasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource gd.Guard.from_pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we construct the `Rail` object directly, we can see that it builds out the `output_schema` and updates the prompts for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system prompt \n",
      "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
      "\n",
      "user prompt \n",
      "${query}\n",
      "\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "<output>\n",
      "    <integer name=\"value\" description=\"The answer to the question.\"/>\n",
      "</output>\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rail = gd.Rail.from_pydantic(\n",
    "    instructions=system_instructions,\n",
    "    prompt=user_prompt,\n",
    "    output_class=IntegerAnswer,\n",
    ")\n",
    "\n",
    "print(\"system prompt\", rail.instructions)\n",
    "print(\"user prompt\", rail.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect `gd.Rail.from_pydantic` more closely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pydantic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minstructions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreask_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreask_instructions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mxml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_xml_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutput_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreask_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreask_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreask_instructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreask_instructions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource gd.Rail.from_pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It relies on a core function called `generate_xml_code` that will produce XML code from the pydantic model. Here is a the docstring for `generate_xml_code` - note that it calls the generated XML code - the XML RAIL Spec - RAIL is short for \"Reliable AI Language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate XML RAIL Spec from a pydantic model and a prompt.\n",
      "\n",
      "    Parameters: Arguments:\n",
      "        prompt (str): The prompt for this RAIL spec.\n",
      "        output_class (BaseModel, optional): The Pydantic model that represents the desired output schema.  Do not specify if using a string schema. Defaults to None.\n",
      "        instructions (str, optional): Instructions for chat models. Defaults to None.\n",
      "        reask_prompt (str, optional): An alternative prompt to use during reasks. Defaults to None.\n",
      "        reask_instructions (str, optional): Alternative instructions to use during reasks. Defaults to None.\n",
      "        validators (List[Validator], optional): The list of validators to apply to the string schema. Do not specify if using a Pydantic model. Defaults to None.\n",
      "        description (str, optional): The description for a string schema. Do not specify if using a Pydantic model. Defaults to None.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from guardrails.rail import generate_xml_code\n",
    "\n",
    "print(generate_xml_code.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our built Guard object to prompt a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the guard object to call our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Happy Path: Our system is able to parse the JSON and return the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 2}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore the GuardrailsAI UserWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    \n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=0,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "validated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guard object's `__call__` will perform the following steps:\n",
    "- Update the prompt by replacing any remaining `${var}` with the appropriate values provided in `prompt_params`\n",
    "- Call the language model API\n",
    "- Parse the returned output using the schema\n",
    "- If the output doesn't match the schema\n",
    "    - it will proceed to perfrom a corrective action\n",
    "        - By default the corrective action is to re-prompt the model asking it to resolve the issue \n",
    "    - it will repeat this process until the output matches the schema or until a maximum number of attempts is reached.\n",
    "- The result is returned as both a string and a structured object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnHappy Path: Our system is able to parse the JSON and return the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's mock our language model API to force a failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkeletonReAsk(incorrect_value={'value': 'the answer is 2'},\n",
      "     fail_results=[FailResult(outcome='fail', metadata=None, error_message='JSON does\n",
      "     not match schema', fix_value=None)])\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "magic_mock = MagicMock()\n",
    "magic_mock.return_value = {\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"the answer is 2\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=1,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "print(\"\\n     \".join(wrapper.wrap(repr(validated_output))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced features in guardrails\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnHappy Path: Introducing Guardrails validators and on-fail actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what options do we have when the model fails to produce the output we want? Can we customize the corrective action? Can we do this on a per attribute basis?\n",
    "\n",
    "The answer is yes. We can define a guardrails validator for each attribute in our schema. The validator will be called on the parsed output and will return a boolean indicating whether the output is valid or not. If the output is not valid, we can define an on-fail action to perform. The on-fail action can be one of the [following actions](https://docs.guardrailsai.com/concepts/output/#specifying-corrective-actions):\n",
    "\n",
    "- `reask`\tReask the LLM to generate an output that meets the quality criteria. The prompt used for reasking contains information about which quality criteria failed, which is auto-generated by the validator.\n",
    "- `fix`\tProgrammatically fix the generated output to meet the quality criteria. E.g. for the formatter two-words, the programatic fix simply takes the first 2 words of the generated string.\n",
    "- `filter`\tFilter the incorrect value. This only filters the field that fails, and will return the rest of the generated output.\n",
    "- `refrain`\tRefrain from returning an output. If a formatter has the corrective action refrain, then on failure there will be a None output returned instead of the JSON.\n",
    "- `noop`\tDo nothing. The failure will still be recorded in the logs, but no corrective action will be taken.\n",
    "- `exception`\tRaise an exception when validation fails.\n",
    "- `fix_reask`\tFirst, fix the generated output deterministically, and then rerun validation with the deterministically fixed output. If validation fails, then perform reasking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the `ValidRange` validator with an `exception` on_fail action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails.validators import ValidRange\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    value: int = Field(\n",
    "        description=\"The answer to the question.\",\n",
    "        validators=[ValidRange(min=0, on_fail=\"exception\")],\n",
    "    )\n",
    "\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    output_class=Answer, prompt=user_prompt, instructions=system_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the language model returns a value that is not a positive integer, an exception will be raised. Let's mock the model to return a negative integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'guardrails.validators.ValidatorError'>\n"
     ]
    }
   ],
   "source": [
    "magic_mock.return_value = {\n",
    "    \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"-2\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    try:\n",
    "        raw_llm_output, validated_output = guard(\n",
    "            llm_api=openai.Completion.create,\n",
    "            prompt_params={\"query\": query},\n",
    "            num_reasks=1,\n",
    "            engine=\"text-davinci-003\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see a ValidationError is now raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we inspect the `re-ask` on-fail action behavior by mocking our model to first return an invalid answer, and then a valid answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 2}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Answer(BaseModel):\n",
    "    value: int = Field(\n",
    "        description=\"The answer to the question.\",\n",
    "        validators=[ValidRange(min=0, max=10, on_fail=\"reask\")],\n",
    "    )\n",
    "\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    output_class=Answer, prompt=user_prompt, instructions=system_instructions\n",
    ")\n",
    "\n",
    "magic_mock = MagicMock()\n",
    "magic_mock.side_effect = [\n",
    "    {\n",
    "        \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"index\": 0,\n",
    "                \"text\": '{\"value\": \"-2\"}',\n",
    "                \"finish_reason\": \"stop\",\n",
    "            }\n",
    "        ],\n",
    "        \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"index\": 0,\n",
    "                \"text\": '{\"value\": \"2\"}',\n",
    "                \"finish_reason\": \"stop\",\n",
    "            }\n",
    "        ],\n",
    "        \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=1,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "validated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasking resolved the issue and returned the correct value ! Lets inpsect the reask prompt that was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\n",
      "\n",
      "\n",
      "\n",
      "I was given the following JSON response, which had problems due to incorrect values.\n",
      "\n",
      "{\n",
      "  \"value\": {\n",
      "    \"incorrect_value\": -2,\n",
      "    \"error_messages\": [\n",
      "      \"Value -2 is less than 0.\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Help me correct the incorrect values based on the given error messages.\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "<output>\n",
      "    <integer name=\"value\" format=\"valid-range: min=0 max=10\" description=\"The answer to the question.\"/>\n",
      "</output>\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.\n",
      "\n",
      "\n",
      "Json Output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(magic_mock.call_args[1][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for some reason Guardrails AI doesn't chose to add the initial query 1+1=? to the prompt which is strange given it should be important context for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we used `fix` instead of `reask` then validator will try to coerce a value of 0 or 10 depending on which is closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': '0'}\n",
      "{'value': '10'}\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseModel):\n",
    "    value: int = Field(\n",
    "        description=\"The answer to the question.\",\n",
    "        validators=[ValidRange(min=0, max=10, on_fail=\"fix\")],\n",
    "    )\n",
    "\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    output_class=Answer, prompt=user_prompt, instructions=system_instructions\n",
    ")\n",
    "\n",
    "magic_mock = MagicMock()\n",
    "magic_mock.return_value = {\n",
    "    \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"-2\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=1,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "print(validated_output)\n",
    "\n",
    "\n",
    "\n",
    "magic_mock.return_value = {\n",
    "    \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"14\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=1,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "print(validated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we use `filter` as the corrective action will return an empty dictionary given we only inspect one key named \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "class Answer(BaseModel):\n",
    "    value: int = Field(\n",
    "        description=\"The answer to the question.\",\n",
    "        validators=[ValidRange(min=0, max=10, on_fail=\"filter\")],\n",
    "    )\n",
    "\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    output_class=Answer, prompt=user_prompt, instructions=system_instructions\n",
    ")\n",
    "\n",
    "magic_mock = MagicMock()\n",
    "magic_mock.return_value = {\n",
    "    \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"-2\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=1,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "print(validated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for a more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing between two possible response schemas\n",
    "\n",
    "For instance if you have a language model that can return more than one possible schema, you can use a choice validator to route between the schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex validators out of the box\n",
    "\n",
    "For certain cases like checking if the returned output is valid SQL or valid Python, you can use the built-in guardrail validators for these cases.\n",
    "\n",
    "see the guardrails [validators page](https://docs.guardrailsai.com/api_reference/validators/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom validators\n",
    "Earlier this year there was a [popular video of how ChatGPT couldn't stick to performing legal chess moves](https://www.youtube.com/watch?v=iWhlrkfJrCQ&ab_channel=GothamChess). Guardrails AI has an [example in progress](https://docs.guardrailsai.com/examples/valid_chess_moves/) of how to use custom validators to enforce a legal chess game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexibility of the guardrails approach\n",
    "The guardrails approach is very flexible and can be used to validate any kind of language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using guardrails to validate a gpt2 model loaded locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using guardrails against the anyscale API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of Guardrails\n",
    "- Given that guardrails relies on re-prompts to correct the model, it is not suitable for use cases where the model is expensive to call. \n",
    "- Default prompts provided by guardrails might not be optimal for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas of improvement\n",
    "\n",
    "- Inheriting validators from pydantic models would be nice but support for it is still lacking.\n",
    "- Using different models to perform correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structured-output-llm-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
