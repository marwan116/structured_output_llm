{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Making Large Language Models Reliable using Guardrails AI\"\n",
    "jupyter: python3\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "filters:\n",
    "  - line-highlight\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models are not reliable\n",
    "\n",
    "I asked GPT-3 to define what \"reliable software\" is. Here's what it said:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliable software can be defined as software that consistently\n",
      "performs its intended functions accurately and efficiently, without\n",
      "any unexpected failures or errors. It is dependable, trustworthy, and\n",
      "can be relied upon to deliver consistent results under various\n",
      "conditions and user interactions. Reliable software is robust, stable,\n",
      "and resilient, ensuring that it operates as expected even in the\n",
      "presence of unforeseen circumstances or changes in the environment. It\n",
      "is also maintainable, allowing for easy updates, bug fixes, and\n",
      "enhancements without compromising its reliability.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=70, break_long_words=False, replace_whitespace=False)\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do you define reliable software?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"\\n\".join(wrapper.wrap(response[\"choices\"][0][\"message\"][\"content\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is safe to say that Language models like GPT-3 don't meet the criteria of reliable software, at least when prompted and used naively. For instance, let's take this very simple prompt of adding two numbers. In this case, I would like gpt-3 to act as a calculator and return back to me the result. Perhaps this is too contrived but for many tasks, we will require the model to consistently use a given output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The integer answer to 1+1 is 2.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: TRUE\n",
    "#| eval: FALSE\n",
    "#| source-line-numbers: 6\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"user\", \"content\": \"Return only the integer answer, 1+1=\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response['choices'][0][\"message\"]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even though I explicitly asked \"Return only the integer answer\" expecting only `2` to be returned but the model returned a full sentence string instead.\n",
    "\n",
    "When designing a system where a language model is one component, how can we adapt it to make it more reliable?\n",
    "\n",
    "How can we enforce an output interface for the LLM model to adhere to (without having to muck around with different prompting strategies manually)?\n",
    "\n",
    "There seems to be several patterns that are emerging:\n",
    "\n",
    "- Use a tool like [guardrails AI](https://www.guardrails.ai/) where one can specify the format using the RAIL spec, prompting the model with a RAIL spec and taking corrective action if the model doesn't adhere to the spec.\n",
    "- Use \"function calling\" capabilities of a few closed-source chat APIs like [OpenAI's API](https://openai.com/blog/function-calling-and-other-api-updates) along with integrations with pydantic (see [askmarvinai](https://www.askmarvin.ai/welcome/overview/) and [instructor](https://jxnl.github.io/instructor/))\n",
    "-  Use a tool like [outlines](https://github.com/outlines-dev/outlines) where one can specify the output format as a regex, JSON schema or pydantic model and outlines will perform a regex-guided generation of the output by modifying the generated model probabilities of tokens so as to adhere to the regex. \n",
    "\n",
    "I am going to examine the guardrails AI approach in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails AI overview\n",
    "\n",
    "### Overview\n",
    "Let's start with [guardrails AI](https://docs.guardrailsai.com/). Using the same query we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=?\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"1+1=?\"\"\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the desired answer format/schema using a popular python library pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class IntegerAnswer(BaseModel):\n",
    "    \"\"\"The answer to a question.\"\"\"\n",
    "\n",
    "    value: int = Field(description=\"The answer to the question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write this guardrails code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "${query}\n",
    "\n",
    "${gr.complete_json_suffix_v2}\n",
    "\"\"\"\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    instructions=instructions,\n",
    "    prompt=prompt,\n",
    "    output_class=IntegerAnswer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails will build the prompt for us given a prompt template that we had to craft.\n",
    "\n",
    "For crafting the prompt-template, we make use of\n",
    "- variables like `query` which we pass in like so `${query}`\n",
    "- constants like `complete_json_suffix_v2` which reference pre-defined prompt templates which we can find in [constants.xml](https://github.com/guardrails-ai/guardrails/blob/main/guardrails/constants.xml) file\n",
    "\n",
    "Let's inspect the prompt that guardrails generated for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
      "\n",
      "\n",
      "${query}\n",
      "\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "<output>\n",
      "    <integer name=\"value\" description=\"The answer to the question.\"/>\n",
      "</output>\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(guard.instructions.source)\n",
    "print(guard.prompt.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the guard object to call our language model. \n",
    "\n",
    "The guard object is a wrapper around the language model that will perform the following steps:\n",
    "- Prepare the prompt using the template and variables\n",
    "- Call the language model\n",
    "- Parse the output using the schema\n",
    "- If the output doesn't match the schema\n",
    "    - it will proceed to perfrom a corrective action\n",
    "        - By default the corrective action is to re-prompt the model asking it to resolve the issue \n",
    "    - it will repeat this process until the output matches the schema or until a maximum number of attempts is reached.\n",
    "- The result is returned as both a string and a structured object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now let's try it out! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 2}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore the UserWarning about Instructions do not have any variables\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=0,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "validated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's mock the case when our language model will return a non-JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_llm_output='{\"value\": \"the answer is 2\"}'\n",
      "SkeletonReAsk(incorrect_value={'value': 'the answer is 2'},\n",
      "fail_results=[FailResult(outcome='fail', metadata=None,\n",
      "error_message='JSON does not match schema', fix_value=None)])\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "magic_mock = MagicMock()\n",
    "magic_mock.return_value = {\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"the answer is 2\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    raw_llm_output, validated_output = guard(\n",
    "        llm_api=openai.Completion.create,\n",
    "        prompt_params={\"query\": query},\n",
    "        num_reasks=1,\n",
    "        engine=\"text-davinci-003\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "print(f\"{raw_llm_output=}\")\n",
    "\n",
    "print(\"\\n\".join(wrapper.wrap(repr(validated_output))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned response now indicates a failure due to an incorrect value. What happened is the first prompt looks exactly like the one we used above, but the second prompt is different. It is a prompt that is asking the model to resolve the issue by returning a JSON response.\n",
    "\n",
    "Given we mocked the model to return a non-JSON response, the model will fail to resolve the issue and will return a failure response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant only capable of communicating with valid JSON, and no other text.\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.\n",
      "\n",
      "Here are examples of simple (XML, JSON) pairs that show the expected behavior:\n",
      "- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\n",
      "- `<list name='bar'><string format='upper-case' /></list>` => `{\"bar\": ['STRING ONE', 'STRING TWO', etc.]}`\n",
      "- `<object name='baz'><string name=\"foo\" format=\"capitalize two-words\" /><integer name=\"index\" format=\"1-indexed\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\n",
      "\n",
      "\n",
      "\n",
      "I was given the following JSON response, which had problems due to incorrect values.\n",
      "\n",
      "{\n",
      "  \"incorrect_value\": {\n",
      "    \"value\": \"the answer is 2\"\n",
      "  },\n",
      "  \"error_messages\": [\n",
      "    \"JSON does not match schema\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Help me correct the incorrect values based on the given error messages.\n",
      "\n",
      "Given below is XML that describes the information to extract from this document and the tags to extract it into.\n",
      "\n",
      "<output>\n",
      "    <integer name=\"value\" description=\"The answer to the question.\"/>\n",
      "</output>\n",
      "\n",
      "\n",
      "ONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.\n",
      "\n",
      "\n",
      "Json Output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(wrapper.wrap(magic_mock.call_args_list[0].kwargs[\"prompt\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced features in guardrails\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce validators and corrective action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more involved validation than the return type, you can use the validators that guardrails provides out of the box, or we can define our own validators. To show how this works, let's define a validator that checks that the sum of the two numbers is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails.validators import ValidRange\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    value: int = Field(\n",
    "        description=\"The answer to the question.\",\n",
    "        validators=[ValidRange(min=0, on_fail=\"exception\")],\n",
    "    )\n",
    "\n",
    "\n",
    "guard = gd.Guard.from_pydantic(\n",
    "    output_class=Answer, prompt=prompt, instructions=instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the language model returns a value that is not a positive integer, an exception will be raised. Let's mock the model to return a negative integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'guardrails.validators.ValidatorError'>\n"
     ]
    }
   ],
   "source": [
    "magic_mock = MagicMock()\n",
    "magic_mock.return_value = {\n",
    "    \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"text\": '{\"value\": \"-2\"}',\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\"prompt_tokens\": 18, \"completion_tokens\": 12, \"total_tokens\": 30},\n",
    "}\n",
    "\n",
    "with patch(\"openai.Completion.create\", magic_mock):\n",
    "    try:\n",
    "        raw_llm_output, validated_output = guard(\n",
    "            llm_api=openai.Completion.create,\n",
    "            prompt_params={\"query\": query},\n",
    "            num_reasks=1,\n",
    "            engine=\"text-davinci-003\",\n",
    "            max_tokens=1024,\n",
    "            temperature=0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex validators out of the box\n",
    "\n",
    "For certain cases like checking if the returned output is valid SQL or valid Python, you can use the built-in guardrail validators for these cases.\n",
    "\n",
    "see the guardrails [validators page](https://docs.guardrailsai.com/api_reference/validators/) for more details.\n",
    "\n",
    "### Custom validators\n",
    "Earlier this year there was a [popular video of how ChatGPT couldn't stick to performing legal chess moves](https://www.youtube.com/watch?v=iWhlrkfJrCQ&ab_channel=GothamChess). Guardrails AI has an [example in progress](https://docs.guardrailsai.com/examples/valid_chess_moves/) of how to use custom validators to enforce a legal chess game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing between two possible response schemas\n",
    "\n",
    "For instance if you have a language model that can return more than one possible schema, you can use a choice validator to route between the schemas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexibility of the guardrails approach\n",
    "The guardrails approach is very flexible and can be used to validate any kind of language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using guardrails to validate a gpt2 model loaded locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using guardrails against the anyscale API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of Guardrails\n",
    "- Given that guardrails relies on re-prompts to correct the model, it is not suitable for use cases where the model is expensive to call. \n",
    "- Default prompts provided by guardrails might not be optimal for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas of improvement\n",
    "\n",
    "- Inheriting validators from pydantic models would be nice but support for it is still lacking.\n",
    "- Using different models to perform correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2764988694.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    gd.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl at 0x111287ce0> JSON: {\n",
       "  \"id\": \"chatcmpl-8CazZKUCp8KbiUt49J5x7eINiMlvl\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1698012825,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"The integer answer to 1+1 is 2.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 18,\n",
       "    \"completion_tokens\": 12,\n",
       "    \"total_tokens\": 30\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_llm_output, validated_output = guard(\n",
    "    openai.Completion.create,\n",
    "    prompt_params={\"doctors_notes\": doctors_notes},\n",
    "    engine=\"text-davinci-003\",\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PatientInfo(gender=\"a random string\", age=100241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientInfo(gender='Male', age=49)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PatientInfo.parse_raw(raw_llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientInfo(gender='Male', age=49)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PatientInfo.parse_obj(validated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=meta-llama/Llama-2-70b-chat-hf-5a7926c5dee748e5b83600f28f3b116c at 0x116d00e00> JSON: {\n",
       "  \"id\": \"meta-llama/Llama-2-70b-chat-hf-5a7926c5dee748e5b83600f28f3b116c\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1697433235,\n",
       "  \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \" Sure! 1 + 1 = 2.\"\n",
       "      },\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 18,\n",
       "    \"completion_tokens\": 12,\n",
       "    \"total_tokens\": 30\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"user\", \"content\": \"Return an integer, 1+1=.\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! 1 + 1 = 2.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_base = \"https://api.openai.com/v1/\"\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8AAGsj9aV2yuw2frp9YLGGZKycWqH at 0x116d65b70> JSON: {\n",
       "  \"id\": \"chatcmpl-8AAGsj9aV2yuw2frp9YLGGZKycWqH\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1697433454,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"The integer expression 1+1 is equal to 2.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 16,\n",
       "    \"completion_tokens\": 13,\n",
       "    \"total_tokens\": 29\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"user\", \"content\": \"Return an integer, 1+1=.\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8AAGZGYbMM6OOHTjdTIrqKIKyJlBs at 0x116c9a930> JSON: {\n",
       "  \"id\": \"chatcmpl-8AAGZGYbMM6OOHTjdTIrqKIKyJlBs\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1697433435,\n",
       "  \"model\": \"gpt-4-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"2\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 16,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 17\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"You are a helpful assistant.\",\n",
    "            \"role\": \"user\", \"content\": \"Return an integer, 1+1=.\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/marwansarieddine/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import outlines.text.generate as generate\n",
    "import outlines.models as models\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02633031351812286487554313921229487190791897\n"
     ]
    }
   ],
   "source": [
    "# model = models.transformers(\"meta-llama/Llama-2-7b-chat-hf\", device=\"mps\")\n",
    "\n",
    "# prompt = \"\"\"1+1=\"\"\"\n",
    "# answer = generate.integer(model, max_tokens=20)(prompt)\n",
    "# print(answer)\n",
    "\n",
    "\n",
    "model = models.transformers(\"gpt2\", device=\"mps\")\n",
    "\n",
    "prompt = \"\"\"1+1=\"\"\"\n",
    "answer = generate.integer(model, max_tokens=20)(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"1+1=\"\"\"\n",
    "answer = generate.integer(model, max_tokens=1)(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.transformers(\"gpt2\", device=\"mps\")\n",
    "\n",
    "prompt = \"\"\"1+1=\"\"\"\n",
    "answer = generate.integer(model, max_tokens=20)(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structured-output-llm-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
